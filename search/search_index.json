{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"modelkit is a Python framework to maintain and run machine learning (ML) code in production environments. The key features are: type-safe Models' inputs and outputs can be validated by pydantic composable Models are composable : they can depend on other models. organized Store and share your models as regular Python packages. extensible Models can rely on arbitrary supporting configurations files called assets hosted on local or cloud object stores testable Models carry their own unit test cases, and unit testing fixtures are available for pytest fast to code Models can be served in a single CLI call using fastapi fast Models' predictions can be batched for speed async Models support async and synchronous prediction functions","title":"Home"},{"location":"cli.html","text":"modelkit CLI \u00b6 Models description \u00b6 Describe \u00b6 This CLI prints out all relevant information on a given modelkit model repository: modelkit describe [ PACKAGE ] [ --required-models ... ] Assets listing \u00b6 This CLI will show all necessary assets to run models modelkit list-assets [ PACKAGE ] [ --required-models ... ] Dependencies graph \u00b6 This CLI will create a .DOT file with a graph of all models, their assets and model dependencies. modelkit dependencies-graph [ PACKAGE ] [ --required-models ... ] This requires graphviz for the graph layout. Benchmarking \u00b6 Memory benchmark \u00b6 This CLI attempts to measure the memory consumption of a set of modelkit models: modelkit memory [ PACKAGE ] [ --required-models ... ] Time \u00b6 This CLI accepts a model and item, and will time the prediction modelkit time MODEL_NAME EXAMPLE_ITEM --models PACKAGE Serving \u00b6 modelkit provides a single CLI to run a local FastAPI server with all loaded models mounted as endpoints: modelkit serve PACKAGE [ --required-models ... ] This is useful in order to inspect the swagger. Important Note that models whose payloads are not serializable will not be exposed, this is true in particular of numpy arrays Assets management \u00b6 To list all assets: $ modelkit assets list To create a new asset: $ modelkit assets new /path/to/asset asset_category/asset_name To update an asset's minor version: $ modelkit assets update /path/to/asset asset_category/asset_name To push a new major version: $ modelkit assets update /path/to/asset asset_category/asset_name --bump-major TF serving \u00b6 To configure models from a package to be run in TF serving: modelkit tf-serving local-docker --models [ PACKAGE ] This will write a configuration file with relative paths to the model files. This is meant to be used by mounting the MODELKIT_ASSETS_DIR in the container under the path /config . Other options include: - local-process To create a config file with absolute paths to the assets under MODELKIT_ASSETS_DIR - remote which will use whichever remote paths are found for the assets (i.e. as configured by the MODELKIT_STORAGE_PROVIDER )","title":"CLI"},{"location":"cli.html#modelkit-cli","text":"","title":"modelkit CLI"},{"location":"cli.html#models-description","text":"","title":"Models description"},{"location":"cli.html#describe","text":"This CLI prints out all relevant information on a given modelkit model repository: modelkit describe [ PACKAGE ] [ --required-models ... ]","title":"Describe"},{"location":"cli.html#assets-listing","text":"This CLI will show all necessary assets to run models modelkit list-assets [ PACKAGE ] [ --required-models ... ]","title":"Assets listing"},{"location":"cli.html#dependencies-graph","text":"This CLI will create a .DOT file with a graph of all models, their assets and model dependencies. modelkit dependencies-graph [ PACKAGE ] [ --required-models ... ] This requires graphviz for the graph layout.","title":"Dependencies graph"},{"location":"cli.html#benchmarking","text":"","title":"Benchmarking"},{"location":"cli.html#memory-benchmark","text":"This CLI attempts to measure the memory consumption of a set of modelkit models: modelkit memory [ PACKAGE ] [ --required-models ... ]","title":"Memory benchmark"},{"location":"cli.html#time","text":"This CLI accepts a model and item, and will time the prediction modelkit time MODEL_NAME EXAMPLE_ITEM --models PACKAGE","title":"Time"},{"location":"cli.html#serving","text":"modelkit provides a single CLI to run a local FastAPI server with all loaded models mounted as endpoints: modelkit serve PACKAGE [ --required-models ... ] This is useful in order to inspect the swagger. Important Note that models whose payloads are not serializable will not be exposed, this is true in particular of numpy arrays","title":"Serving"},{"location":"cli.html#assets-management","text":"To list all assets: $ modelkit assets list To create a new asset: $ modelkit assets new /path/to/asset asset_category/asset_name To update an asset's minor version: $ modelkit assets update /path/to/asset asset_category/asset_name To push a new major version: $ modelkit assets update /path/to/asset asset_category/asset_name --bump-major","title":"Assets management"},{"location":"cli.html#tf-serving","text":"To configure models from a package to be run in TF serving: modelkit tf-serving local-docker --models [ PACKAGE ] This will write a configuration file with relative paths to the model files. This is meant to be used by mounting the MODELKIT_ASSETS_DIR in the container under the path /config . Other options include: - local-process To create a config file with absolute paths to the assets under MODELKIT_ASSETS_DIR - remote which will use whichever remote paths are found for the assets (i.e. as configured by the MODELKIT_STORAGE_PROVIDER )","title":"TF serving"},{"location":"configuration.html","text":"Configuration \u00b6 Environment \u00b6 In order to run/deploy modelkit endpoints, you need to provide it with the necessary environment variables, most of them required by modelkit.assets to retrieve assets from the remote object store: MODELKIT_STORAGE_BUCKET (default: unset): override storage container where assets are retrieved from. MODELKIT_ASSETS_DIR : the local directory in which assets will be downloaded and cached. This needs to be a valid local directory. MODELKIT_STORAGE_PROVIDER (default: gcs ) the storage provider (does not have to be set) for MODELKIT_STORAGE_PROVIDER=gcs , the variable GOOGLE_APPLICATION_CREDENTIALS need to be pointing to a service account credentials JSON file (this is not necessary on dev machines) for MODELKIT_STORAGE_PROVIDER=s3ssm , you need to instantiate AWS_PROFILE Refer to the AssetsManager settings documentation for more information. MODELKIT_LAZY_LOADING (defaults to False ) toggles lazy loading mode for the ModelLibrary MODELKIT_TF_SERVING_ENABLE (default: True ): Get tensorflow data from tensorflow server, instead of loading these data locally (if set to False you need to install tensorflow). MODELKIT_TF_SERVING_HOST (default: localhost ): IP address of tensorflow server MODELKIT_TF_SERVING_PORT (default: 8501 ): Port of tensorflow server MODELKIT_TF_SERVING_MODE (default: rest ): rest to use REST protocol of tensorflow server (port 8501), grpc to use GRPC protocol (port 8500) TF_SERVING_TIMEOUT_S (default: 60 ): Timeout duration for tensorflow server calls MODELKIT_CACHE_PROVIDER (default: None ) to use prediction caching if MODELKIT_CACHE_PROVIDER=redis , use an external redis instance for caching: MODELKIT_CACHE_HOST (default: localhost ) MODELKIT_CACHE_PORT (default: 6379 ) if MODELKIT_CACHE_PROVIDER=native use native caching (via cachetools ): MODELKIT_CACHE_IMPLEMENTATION can be MODELKIT_CACHE_MAX_SIZE size of the cache modelkit_ASYNC_MODE (default to None ) forces the DistantHTTPModels to use async mode. New Assets Testing Environment \u00b6 Developer can test new assets without having to add them to ASSET_PREFIX . OVERRIDE_ASSET_PREFIX override ASSET_PREFIX . modelkit will firstly try to download the assets from OVERRIDE_ASSET_PREFIX (in which the developper pushed his new assets to test) before falling back to ASSET_PREFIX Dependencies \u00b6 modelkit requires Python 3.7. You can use pyenv or anaconda to install it. We recommend using a virtual environment. Then, in order to install modelkit dependencies, use: pip install -r requirements.txt Or to also install the developer tools (testing, etc.): pip install -r requirements-dev.txt If necessary, you have to install manually the tensorflow dependency: pip install modelkit[tensorflow]","title":"Configuration"},{"location":"configuration.html#configuration","text":"","title":"Configuration"},{"location":"configuration.html#environment","text":"In order to run/deploy modelkit endpoints, you need to provide it with the necessary environment variables, most of them required by modelkit.assets to retrieve assets from the remote object store: MODELKIT_STORAGE_BUCKET (default: unset): override storage container where assets are retrieved from. MODELKIT_ASSETS_DIR : the local directory in which assets will be downloaded and cached. This needs to be a valid local directory. MODELKIT_STORAGE_PROVIDER (default: gcs ) the storage provider (does not have to be set) for MODELKIT_STORAGE_PROVIDER=gcs , the variable GOOGLE_APPLICATION_CREDENTIALS need to be pointing to a service account credentials JSON file (this is not necessary on dev machines) for MODELKIT_STORAGE_PROVIDER=s3ssm , you need to instantiate AWS_PROFILE Refer to the AssetsManager settings documentation for more information. MODELKIT_LAZY_LOADING (defaults to False ) toggles lazy loading mode for the ModelLibrary MODELKIT_TF_SERVING_ENABLE (default: True ): Get tensorflow data from tensorflow server, instead of loading these data locally (if set to False you need to install tensorflow). MODELKIT_TF_SERVING_HOST (default: localhost ): IP address of tensorflow server MODELKIT_TF_SERVING_PORT (default: 8501 ): Port of tensorflow server MODELKIT_TF_SERVING_MODE (default: rest ): rest to use REST protocol of tensorflow server (port 8501), grpc to use GRPC protocol (port 8500) TF_SERVING_TIMEOUT_S (default: 60 ): Timeout duration for tensorflow server calls MODELKIT_CACHE_PROVIDER (default: None ) to use prediction caching if MODELKIT_CACHE_PROVIDER=redis , use an external redis instance for caching: MODELKIT_CACHE_HOST (default: localhost ) MODELKIT_CACHE_PORT (default: 6379 ) if MODELKIT_CACHE_PROVIDER=native use native caching (via cachetools ): MODELKIT_CACHE_IMPLEMENTATION can be MODELKIT_CACHE_MAX_SIZE size of the cache modelkit_ASYNC_MODE (default to None ) forces the DistantHTTPModels to use async mode.","title":"Environment"},{"location":"configuration.html#new-assets-testing-environment","text":"Developer can test new assets without having to add them to ASSET_PREFIX . OVERRIDE_ASSET_PREFIX override ASSET_PREFIX . modelkit will firstly try to download the assets from OVERRIDE_ASSET_PREFIX (in which the developper pushed his new assets to test) before falling back to ASSET_PREFIX","title":"New Assets Testing Environment"},{"location":"configuration.html#dependencies","text":"modelkit requires Python 3.7. You can use pyenv or anaconda to install it. We recommend using a virtual environment. Then, in order to install modelkit dependencies, use: pip install -r requirements.txt Or to also install the developer tools (testing, etc.): pip install -r requirements-dev.txt If necessary, you have to install manually the tensorflow dependency: pip install modelkit[tensorflow]","title":"Dependencies"},{"location":"assets/index.html","text":"modelkit.assets contains code to create, upload, update and retrieve data assets on the object store. API \u00b6 In order to retrieve an asset: from modelkit.assets.manager import AssetsManager mng = AssetsManager () asset_path = mng . fetch_asset ( \"asset_category/asset_name:version\" ) with open ( asset_path , \"r\" ) as f : # do something Important You need to configure your environment to run this code. CLI \u00b6 To list all assets: $ assets list To create a new asset: $ assets new /path/to/asset asset_category/asset_name To update an asset's minor version: $ assets update /path/to/asset asset_category/asset_name To push a new major version: $ assets update /path/to/asset asset_category/asset_name --bump-major","title":"Introduction"},{"location":"assets/index.html#api","text":"In order to retrieve an asset: from modelkit.assets.manager import AssetsManager mng = AssetsManager () asset_path = mng . fetch_asset ( \"asset_category/asset_name:version\" ) with open ( asset_path , \"r\" ) as f : # do something Important You need to configure your environment to run this code.","title":"API"},{"location":"assets/index.html#cli","text":"To list all assets: $ assets list To create a new asset: $ assets new /path/to/asset asset_category/asset_name To update an asset's minor version: $ assets update /path/to/asset asset_category/asset_name To push a new major version: $ assets update /path/to/asset asset_category/asset_name --bump-major","title":"CLI"},{"location":"assets/assets.html","text":"Assets \u00b6 An asset is a file or a directory that is typically shared and stored on a remote object store (such as GCS or S3). modelkit has tooling to push and retrieve these assets. At runtime, the ModelLibrary will retrieve the necessary assets from the object store, and store them locally. The paths to these assets is then passed to each Model instance which can then load the relevant parts. For developers, since the assets are versioned, they can be persisted locally in an ASSETS_DIR , thus they are only ever downloaded once. AssetsManager \u00b6 An AssetsManager instance can be explicitly created as follows: from modelkit.assets.manager import AssetsManager asset_manager = AssetsManager ( assets_dir = \"/path/to/local/dir\" ) In this case, assets will be looked up locally under the assets_dir . modelkit expects a particular directory structure in which to find the assets. In addition, the AssetsManager can be provided with an storage_prefix which prefixes all stored assets ( modelkit-assets by default). Fetching assets \u00b6 The AssetsManager.fetch_asset method takes a string specification of the asset in the form asset_category/asset_name:version . The specification: may contain exact specification of the version: asset_category/asset_name:1.10 points to the exact 1.10 version may omit the minor version: asset_category/asset_name:1 points to the latest available version 1.* may omit version: asset_category/asset_name points to the latest version *.* Note Keep in mind that the asset name can contain any number of separators ( / ). By default, AssetsManager.fetch_asset only returns the path to the locally downloaded asset, but it can return more information about the fetched asset if provided with the return_info=True . In this case it returns a dictionary with: { \"path\": \"/local/path/to/asset\", \"from_cache\": whether the asset was pulled from cache, \"meta\": {contents of the meta JSON object}, \"version\": \"returned asset version\", \"object_name\": remote object name, \"meta_object_name\": remote meta object name, \"versions_object_name\": remote version object name } Remote asset storage convention \u00b6 Data object \u00b6 Remote assets are stored in object stores, referenced as: [provider]://[bucket]/[assetsmanager-prefix]/[category]/[name]/[version] Note If the asset consists in a directory, all sub files will be stored as separate objects with this prefix. In this \"path\": provider is s3 or gcs or file depending on the storage driver bucket is the remote container name The rest of the \"path\" is the remote object's name and consists of assetsmanager-prefix is a prefix to all assets for a given AssetsManager name describes the asset. The name may contain path separators / but each file remotely will be stored as a single object. version describes the asset version in the form X.Y Meta object \u00b6 In addition to the data, the asset object reside alongside a *.meta object: [provider]://[bucket]/[assetsmanager-prefix]/[category]/[name]/[version].meta The meta is a JSON file containing { \"push_date\": [ISO date of push], \"is_directory\": [bool] \"hash\": [content hash] } Asset compression \u00b6 The behavior of the AssetsManager depends on whether the local asset being pushed is a directory or a file: If it is a file, then it is pushed as-is, without being compressed If it is a directory, it is pushed as a tar.gz file. It is uncompressed upon being fetched, such that the path returned by AssetsManager.fetch_asset is a directory. This information is stored in the metadata file. Version object \u00b6 Assets have versions, following a Major.Minor version convention. We maintain a versions JSON file in order to keep track of the latest version. It is stored at [provider]://[bucket]/[assetsmanager-prefix]/[category]/[name].versions And contains { \"versions\": [list of ordered versions, latest first] }","title":"Development"},{"location":"assets/assets.html#assets","text":"An asset is a file or a directory that is typically shared and stored on a remote object store (such as GCS or S3). modelkit has tooling to push and retrieve these assets. At runtime, the ModelLibrary will retrieve the necessary assets from the object store, and store them locally. The paths to these assets is then passed to each Model instance which can then load the relevant parts. For developers, since the assets are versioned, they can be persisted locally in an ASSETS_DIR , thus they are only ever downloaded once.","title":"Assets"},{"location":"assets/assets.html#assetsmanager","text":"An AssetsManager instance can be explicitly created as follows: from modelkit.assets.manager import AssetsManager asset_manager = AssetsManager ( assets_dir = \"/path/to/local/dir\" ) In this case, assets will be looked up locally under the assets_dir . modelkit expects a particular directory structure in which to find the assets. In addition, the AssetsManager can be provided with an storage_prefix which prefixes all stored assets ( modelkit-assets by default).","title":"AssetsManager"},{"location":"assets/assets.html#fetching-assets","text":"The AssetsManager.fetch_asset method takes a string specification of the asset in the form asset_category/asset_name:version . The specification: may contain exact specification of the version: asset_category/asset_name:1.10 points to the exact 1.10 version may omit the minor version: asset_category/asset_name:1 points to the latest available version 1.* may omit version: asset_category/asset_name points to the latest version *.* Note Keep in mind that the asset name can contain any number of separators ( / ). By default, AssetsManager.fetch_asset only returns the path to the locally downloaded asset, but it can return more information about the fetched asset if provided with the return_info=True . In this case it returns a dictionary with: { \"path\": \"/local/path/to/asset\", \"from_cache\": whether the asset was pulled from cache, \"meta\": {contents of the meta JSON object}, \"version\": \"returned asset version\", \"object_name\": remote object name, \"meta_object_name\": remote meta object name, \"versions_object_name\": remote version object name }","title":"Fetching assets"},{"location":"assets/assets.html#remote-asset-storage-convention","text":"","title":"Remote asset storage convention"},{"location":"assets/assets.html#data-object","text":"Remote assets are stored in object stores, referenced as: [provider]://[bucket]/[assetsmanager-prefix]/[category]/[name]/[version] Note If the asset consists in a directory, all sub files will be stored as separate objects with this prefix. In this \"path\": provider is s3 or gcs or file depending on the storage driver bucket is the remote container name The rest of the \"path\" is the remote object's name and consists of assetsmanager-prefix is a prefix to all assets for a given AssetsManager name describes the asset. The name may contain path separators / but each file remotely will be stored as a single object. version describes the asset version in the form X.Y","title":"Data object"},{"location":"assets/assets.html#meta-object","text":"In addition to the data, the asset object reside alongside a *.meta object: [provider]://[bucket]/[assetsmanager-prefix]/[category]/[name]/[version].meta The meta is a JSON file containing { \"push_date\": [ISO date of push], \"is_directory\": [bool] \"hash\": [content hash] }","title":"Meta object"},{"location":"assets/assets.html#asset-compression","text":"The behavior of the AssetsManager depends on whether the local asset being pushed is a directory or a file: If it is a file, then it is pushed as-is, without being compressed If it is a directory, it is pushed as a tar.gz file. It is uncompressed upon being fetched, such that the path returned by AssetsManager.fetch_asset is a directory. This information is stored in the metadata file.","title":"Asset compression"},{"location":"assets/assets.html#version-object","text":"Assets have versions, following a Major.Minor version convention. We maintain a versions JSON file in order to keep track of the latest version. It is stored at [provider]://[bucket]/[assetsmanager-prefix]/[category]/[name].versions And contains { \"versions\": [list of ordered versions, latest first] }","title":"Version object"},{"location":"assets/assets_dir.html","text":"Assets directory structure \u00b6 The AssetsManager persists assets locally for developmnet: when an asset is requested with AssetsManager.fetch_asset it checks whether an up-to-date version of the asset is present before downloading a new asset if necessary. Assets are considered immutable and versioned, no checks are performed to verify that the objects have not been manually changed locally. Fetching assets \u00b6 The local asset directory is found at ASSETS_DIR , where both values can be fed to the AssetsManager at initialization. Each asset's name is splitted along the path separators as directories, and version information is added. For example, we have pushed to the remote store two assets: a directory to some/directory/asset and a file to some/asset . After retrieving them to the assets_dir , it will look like this: assets_dir \u2514\u2500\u2500 some | \u251c\u2500\u2500 asset | \u2502 \u251c\u2500\u2500 0.0 | \u2502 \u251c\u2500\u2500 0.1 | \u2502 | ... | \u251c\u2500\u2500 directory | \u2502 \u251c\u2500\u2500 asset | \u2502 \u2502 \u251c\u2500\u2500 0.0 | \u2502 \u2502 | \u251c\u2500\u2500 content0 | \u2502 \u2502 | \u251c\u2500\u2500 content2 | \u2502 \u2502 | | ... | \u2502 \u2502 \u251c\u2500\u2500 0.1 | \u2502 \u2502 | | ... | ... To retrieve the assets path, it is enough to run: mng = AssetsManager ( assets_dir = assets_dir ) mng . fetch_asset ( \"some/asset:0.0\" ) # will point to `some/asset/0.0` mng . fetch_asset ( \"some/directory/asset:0.1\" ) # will point to `some/asset/0.1`","title":"Local cache"},{"location":"assets/assets_dir.html#assets-directory-structure","text":"The AssetsManager persists assets locally for developmnet: when an asset is requested with AssetsManager.fetch_asset it checks whether an up-to-date version of the asset is present before downloading a new asset if necessary. Assets are considered immutable and versioned, no checks are performed to verify that the objects have not been manually changed locally.","title":"Assets directory structure"},{"location":"assets/assets_dir.html#fetching-assets","text":"The local asset directory is found at ASSETS_DIR , where both values can be fed to the AssetsManager at initialization. Each asset's name is splitted along the path separators as directories, and version information is added. For example, we have pushed to the remote store two assets: a directory to some/directory/asset and a file to some/asset . After retrieving them to the assets_dir , it will look like this: assets_dir \u2514\u2500\u2500 some | \u251c\u2500\u2500 asset | \u2502 \u251c\u2500\u2500 0.0 | \u2502 \u251c\u2500\u2500 0.1 | \u2502 | ... | \u251c\u2500\u2500 directory | \u2502 \u251c\u2500\u2500 asset | \u2502 \u2502 \u251c\u2500\u2500 0.0 | \u2502 \u2502 | \u251c\u2500\u2500 content0 | \u2502 \u2502 | \u251c\u2500\u2500 content2 | \u2502 \u2502 | | ... | \u2502 \u2502 \u251c\u2500\u2500 0.1 | \u2502 \u2502 | | ... | ... To retrieve the assets path, it is enough to run: mng = AssetsManager ( assets_dir = assets_dir ) mng . fetch_asset ( \"some/asset:0.0\" ) # will point to `some/asset/0.0` mng . fetch_asset ( \"some/directory/asset:0.1\" ) # will point to `some/asset/0.1`","title":"Fetching assets"},{"location":"assets/assets_push.html","text":"This section describes how to push assets either manually using CLI or programmatically. General considerations \u00b6 There are two separate actions one can take to affect the remotely stored assets: update an existing asset: If the asset already exists remotely (and, in particular has a .versions object present), the appropriate action is to update it. It is possible to update the minor version (the default behavior), or the major version. create a new asset: If this is the first time that this asset is created, the correct action is the create it, which will assign it the 0.0 version. Maintaining assets programmatically \u00b6 First, instantiate an AssetsManager pointing to the desired bucket , possibly changing the storage_prefix , storage method, etc.: from modelkit.assets.manager import AssetsManager assets_manager = AssetsManager ( bucket =... , ... ) Updating the asset \u00b6 Assuming the asset is locally present at asset_path (either a file or a directory), update the remote asset name as follows: assets_manager . update_asset ( asset_path , name , bump_major = False , major = None ) This will bump the latest existing version's minor version: V.v => V.(v+1) Options: bump_major : If True , will bump the major version of the asset and create a (V+1).0 asset (assuming V is the highest existing major version) major=V : If not falsy, will bump the minor version of the latest asset version with major version V : V.v => V.(v+1) Creating a new asset \u00b6 Assuming the asset is locally present at asset_path (either a file or a directory), create the remote asset name:0.0 as follows: assets_manager . create_asset ( asset_path , name ) Important Creating new assets programmatically is likely not a very good idea. Just do it manually once using the CLI. Maintaining assets with CLI \u00b6 bin/assets.py implements CLIs to ease the maintenance of remote assets. Use --bucket or --assetsmanager-prefix to affect the remote bucket and prefix for the CLI you are using. Alternatively one can also use environment variables as per Environment to affect the remote storage and AssetsManager parameters. Creating a new asset \u00b6 To create a new asset: bin/asset.py new /path/to/asset asset_name After prompting your for confirmation, it will create a remote asset with version 0.0 . Updating the asset \u00b6 Use bin.asset.py update to update an existing asset using a local file or directory at /local/asset/path . Bumping the minor version \u00b6 Assuming name has versions 0.1 , 1.1 , running bin/asset.py update /local/asset/path name will add a version 1.2 Bumping the major version \u00b6 Assuming name has versions 0.1 , 1.0 , running bin/asset.py update /local/asset/path name --bump-major After prompting your for confirmation, it will add a version 2.0 Bumping the minor version of an older asset \u00b6 Assuming name has versions 0.1 , 1.0 , running bin/asset.py update /local/asset/path name:0 will add a version 0.2 Listing remote assets \u00b6 A CLI is available to list remote assets in a given bucket: bin/asset.py list","title":"Assets push"},{"location":"assets/assets_push.html#general-considerations","text":"There are two separate actions one can take to affect the remotely stored assets: update an existing asset: If the asset already exists remotely (and, in particular has a .versions object present), the appropriate action is to update it. It is possible to update the minor version (the default behavior), or the major version. create a new asset: If this is the first time that this asset is created, the correct action is the create it, which will assign it the 0.0 version.","title":"General considerations"},{"location":"assets/assets_push.html#maintaining-assets-programmatically","text":"First, instantiate an AssetsManager pointing to the desired bucket , possibly changing the storage_prefix , storage method, etc.: from modelkit.assets.manager import AssetsManager assets_manager = AssetsManager ( bucket =... , ... )","title":"Maintaining assets programmatically"},{"location":"assets/assets_push.html#updating-the-asset","text":"Assuming the asset is locally present at asset_path (either a file or a directory), update the remote asset name as follows: assets_manager . update_asset ( asset_path , name , bump_major = False , major = None ) This will bump the latest existing version's minor version: V.v => V.(v+1) Options: bump_major : If True , will bump the major version of the asset and create a (V+1).0 asset (assuming V is the highest existing major version) major=V : If not falsy, will bump the minor version of the latest asset version with major version V : V.v => V.(v+1)","title":"Updating the asset"},{"location":"assets/assets_push.html#creating-a-new-asset","text":"Assuming the asset is locally present at asset_path (either a file or a directory), create the remote asset name:0.0 as follows: assets_manager . create_asset ( asset_path , name ) Important Creating new assets programmatically is likely not a very good idea. Just do it manually once using the CLI.","title":"Creating a new asset"},{"location":"assets/assets_push.html#maintaining-assets-with-cli","text":"bin/assets.py implements CLIs to ease the maintenance of remote assets. Use --bucket or --assetsmanager-prefix to affect the remote bucket and prefix for the CLI you are using. Alternatively one can also use environment variables as per Environment to affect the remote storage and AssetsManager parameters.","title":"Maintaining assets with CLI"},{"location":"assets/assets_push.html#creating-a-new-asset_1","text":"To create a new asset: bin/asset.py new /path/to/asset asset_name After prompting your for confirmation, it will create a remote asset with version 0.0 .","title":"Creating a new asset"},{"location":"assets/assets_push.html#updating-the-asset_1","text":"Use bin.asset.py update to update an existing asset using a local file or directory at /local/asset/path .","title":"Updating the asset"},{"location":"assets/assets_push.html#bumping-the-minor-version","text":"Assuming name has versions 0.1 , 1.1 , running bin/asset.py update /local/asset/path name will add a version 1.2","title":"Bumping the minor version"},{"location":"assets/assets_push.html#bumping-the-major-version","text":"Assuming name has versions 0.1 , 1.0 , running bin/asset.py update /local/asset/path name --bump-major After prompting your for confirmation, it will add a version 2.0","title":"Bumping the major version"},{"location":"assets/assets_push.html#bumping-the-minor-version-of-an-older-asset","text":"Assuming name has versions 0.1 , 1.0 , running bin/asset.py update /local/asset/path name:0 will add a version 0.2","title":"Bumping the minor version of an older asset"},{"location":"assets/assets_push.html#listing-remote-assets","text":"A CLI is available to list remote assets in a given bucket: bin/asset.py list","title":"Listing remote assets"},{"location":"assets/environment.html","text":"Environment \u00b6 Model library \u00b6 It is possible to set a default value of a package in which ModelLibrary will look for models: Environment variable Default value Notes MODELKIT_DEFAULT_PACKAGE None It has to be findable (on the PYTHONPATH ) MODELKIT_ASSETS_DIR None Local directory to find assets, has to exist AssetsManager settings \u00b6 The parameters necessary to instantiate an AssetsManager can all be read from environment variables, or provided when initializing the AssetsManager . Environment variable Default value Parameter Notes MODELKIT_STORAGE_PROVIDER gcs storage_provider gcs (default), s3 or local MODELKIT_STORAGE_BUCKET None bucket Bucket in which data is stored MODELKIT_STORAGE_PREFIX modelkit-assets storage_prefix Objects prefix MODELKIT_STORAGE_TIMEOUT_S 300 timeout_s file lock timeout when downloading assets More settings can be passed in order to configure the driver itself. Storage driver settings \u00b6 AWS storage \u00b6 This storage provider is used locally and in production on AWS. We use boto3 under the hood. Then, configure the following variables: Environment variable Value MODELKIT_STORAGE_PROVIDER s3 MODELKIT_STORAGE_BUCKET AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN AWS_DEFAULT_REGION S3_ENDPOINT GCS storage \u00b6 We use google-cloud-storage . Environment variable Default value Notes GOOGLE_APPLICATION_CREDENTIALS None path to the JSON file By default, the GCS client use the credentials setup up on the machine. If GOOGLE_APPLICATION_CREDENTIALS is provided, it should point to a local JSON service account file. Local storage \u00b6 The local storage method is mostly used for development, and should not be useful to end users. It implements a StorageDriver from a local directory, that emulates other object providers. In this case MODELKIT_STORAGE_BUCKET describes a local folder from which assets will be pulled. Prediction Service \u00b6 MODELKIT_STORAGE_PREFIX_OVERRIDE is used to set ModelLibrary override_storage_prefix setting","title":"Environment"},{"location":"assets/environment.html#environment","text":"","title":"Environment"},{"location":"assets/environment.html#model-library","text":"It is possible to set a default value of a package in which ModelLibrary will look for models: Environment variable Default value Notes MODELKIT_DEFAULT_PACKAGE None It has to be findable (on the PYTHONPATH ) MODELKIT_ASSETS_DIR None Local directory to find assets, has to exist","title":"Model library"},{"location":"assets/environment.html#assetsmanager-settings","text":"The parameters necessary to instantiate an AssetsManager can all be read from environment variables, or provided when initializing the AssetsManager . Environment variable Default value Parameter Notes MODELKIT_STORAGE_PROVIDER gcs storage_provider gcs (default), s3 or local MODELKIT_STORAGE_BUCKET None bucket Bucket in which data is stored MODELKIT_STORAGE_PREFIX modelkit-assets storage_prefix Objects prefix MODELKIT_STORAGE_TIMEOUT_S 300 timeout_s file lock timeout when downloading assets More settings can be passed in order to configure the driver itself.","title":"AssetsManager settings"},{"location":"assets/environment.html#storage-driver-settings","text":"","title":"Storage driver settings"},{"location":"assets/environment.html#aws-storage","text":"This storage provider is used locally and in production on AWS. We use boto3 under the hood. Then, configure the following variables: Environment variable Value MODELKIT_STORAGE_PROVIDER s3 MODELKIT_STORAGE_BUCKET AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN AWS_DEFAULT_REGION S3_ENDPOINT","title":"AWS storage"},{"location":"assets/environment.html#gcs-storage","text":"We use google-cloud-storage . Environment variable Default value Notes GOOGLE_APPLICATION_CREDENTIALS None path to the JSON file By default, the GCS client use the credentials setup up on the machine. If GOOGLE_APPLICATION_CREDENTIALS is provided, it should point to a local JSON service account file.","title":"GCS storage"},{"location":"assets/environment.html#local-storage","text":"The local storage method is mostly used for development, and should not be useful to end users. It implements a StorageDriver from a local directory, that emulates other object providers. In this case MODELKIT_STORAGE_BUCKET describes a local folder from which assets will be pulled.","title":"Local storage"},{"location":"assets/environment.html#prediction-service","text":"MODELKIT_STORAGE_PREFIX_OVERRIDE is used to set ModelLibrary override_storage_prefix setting","title":"Prediction Service"},{"location":"library/developing_modelkit.html","text":"Models and assets \u00b6 A modelkit.Model is a Python class implementing methods to deserialize assets (stored in the object store), and use them to make prediction. Some quick Model facts: Models do not have to have an asset (text cleaning models) Models do not have to have a prediction method (they are then called Asset ) Models can depend on other models and share objects in memory (in particular, they can share an Asset ) Models can implement batched/vectorized logic Models can implement asynchronous logic and be called either way Models can implement the logic to fit themselves and generate the asset for prediction The Model class \u00b6 modelkit models are subclasses of the modelkit.core.model.Model class. The prediction logic is implemented in an asynchronous _predict method that takes a single argument item . This represents a single item, which is usually a json serializable dict (with maybe numpy arrays). In fact, Models implement _predict or _predict_batch (both async) methods, and Model appropriately chooses between them and batches. The asset loading logic is implemented in a _load method that is run after the Model is instantiated, and can load the asset specified in the Model 's configuration. For more on this see Lazy Mode. The simplest Model class \u00b6 This simple model class will always return \"something\" : from modelkit.core.model import Model class SimpleModel ( Model ): def _predict ( self , item ) -> str : return \"something\" It can be loaded to make \"predictions\" as so: m = SimpleModel () m ({}) # returns \"something\" Model configurations \u00b6 As models become more complicated they are attached to different assets or other models. We will need to instanciate them through the ModelLibrary object which will take care of all this for us. To do so, we have to configure our model: give it a name and dependencies. Models are made available to clients using modelkit by specifying them using the CONFIGURATIONS class attribute: class SimpleModel ( Model ): CONFIGURATIONS = { \"simple\" : {} } def _predict ( self , item ): return \"something\" Right now, we have only given it a name \"simple\" which makes the model available to other models via the ModelLibrary . The rest of the configuration is empty but we will add to it at the next section. Assuming that SimpleModel is defined in my_module.my_models , it is now accessible via: from modelkit.core import ModelLibrary import my_module.my_models p = ModelLibrary ( models = my_module . my_models ) m = p . get ( \"simple\" ) See Organization for more information on how to organize your models. Model settings \u00b6 The simplest configuration options are model_settings : from modelkit.core.model import Model class SimpleModel ( Model ): CONFIGURATIONS = { \"simple\" : { \"model_settings\" : { \"value\" : \"something\" }}, \"simple2\" : { \"model_settings\" : { \"value\" : \"something2\" }} } def _predict ( self , item ): return self . model_settings [ \"value\" ] Now, there are two versions of the model available, simple and simple2 : from modelkit.core import ModelLibrary p = ModelLibrary ( models = SimpleModel ) m = p . get ( \"simple\" ) print ( m ({})) m2 = p . get ( \"simple2\" ) print ( m2 ({})) It will print both \"something\" and \"something2\" . Model assets and dependencies \u00b6 The usefulness of modelkit Model s and their configuration is more apparent when they depend on assets or other models. Model assets \u00b6 A model can implement a _load method that loads information from an asset stored locally, or retrieved from an object store at run time. It may contain files, folders, parameters, optimized data structures, or anything really. The model asset is specified in the CONFIGURATIONS with asset=asset_name:version , following storage.AssetsManager conventions (see Assets ). When the _load method is called, the object will have an asset_path attribute that points to the path of the asset locally. This is then used to load the relevant information from the asset file(s). Model with asset example \u00b6 Adding the key to the model's configuration: class ModelWithAsset ( Model ): CONFIGURATIONS = { \"model_with_asset\" : { \"asset\" : \"test/yolo:1\" } } Will cause the ModelLibrary to download gs://bucket/assets/test/1/yolo locally to the assets folder folder (which storage provider and bucket is used depends on your configuration), and set the Model.asset_path attribute accordingly. It is up to the user to define the deserialization logic: class ModelWithAsset ( Model ): def _load ( self ): # For example, here, the asset is a BZ2 compressed JSON file with bz2 . BZ2File ( self . asset_path , \"rb\" ) as f : self . data_structure = pickle . load ( f ) # loads {\"response\": \"YOLO les simpsons\"} def _predict ( self , item : Dict [ str , str ], ** kwargs ) -> float : return self . data_structure [ \"response\" ] # returns \"YOLO les simpsons\" Note Assets retrieval is handled by modelkit , and it is guaranteed that they be present when _load is called, even in lazy mode. However, it is not true when __init__ is called. In general, it is not a good idea to subclass __init__ . Model dependencies \u00b6 In addition, modelkit models are composable . That is, a Model can depend on other Model s, and exploit their attributes and predictions. For example your can set your model's configuration to have access to two other Model objects: class SomeModel ( Model ): CONFIGURATIONS = { \"some_model\" : { \"model_dependencies\" : { \"sentence_piece_cleaner\" , \"sentence_piece_vectorizer\" } } } The ModelLibrary ensures that whenever _load or the _predict_* function are called, these models are loaded and present in the model_dependencies dictionary: def _predict ( self , item ): cleaned = self . models_dependencies [ \"sentence_piece_cleaner\" ]( item [ \"text\" ]) ... In addition, it is possible to rename dependencies on the fly by providing a mapping to model_dependencies : class SomeModel ( Model ): CONFIGURATIONS = { \"some_model\" : { \"model_dependencies\" : { \"cleaner\" : \"sentence_piece_cleaner\" , } }, \"some_model_2\" : { \"model_dependencies\" : { \"cleaner\" : \"sentence_piece_cleaner_2 } } } In this case, the instantiated SomeModel.model_dependencies[\"cleaner\"] will point to sentence_piece_cleaner in one configuration and to sentence_piece_cleaner_2 in the other. Model attributes \u00b6 Model have several attributes set by the ModelLibrary when they are initialized: asset_path the path to the asset set in the Model 's configuration configuration_key the key of the model's configuration model_dependencies a dictionary of Model dependencies model_settings the model_settings as passed at initialization service_settings the settings of the ModelLibrary that created the model And a set of attributes always present: model_classname the name of the Model 's subclass batch_size the batch size for the model: if _predict_batch is implemented it will always get batches of this size (defaults to 64) Model typing \u00b6 It is also possible to provide types for a Model subclass, such that linters and callers know exactly which item type is expected, and what the result of a Model call look like. Types are specified when instantiating the Model class: # This model takes `str` items and always returns `int` values class SomeTypedModel ( Model [ str , int ]): def _predict ( self , item ): return len ( item ) Static type checking \u00b6 Setting Model types allows static type checkers to fail if the expected return value of calls to predict have the wrong types. Consider the above model: m = SomeTypedModel () x : int = m ( \"ok\" ) y : List [ int ] = m ([ \"ok\" , \"boomer\" ]) z : int = m ( 1 ) # would lead to a typing error with typecheckers (e.g. mypy) Runtime type validation \u00b6 In addition, whenever the model's predict method is called, the type of the item is validated against the provided type and raises an error if the validation fails: modelkit.core.model.ItemValidationException if the item fails to validate modelkit.core.model.ReturnValueValidationException if the return value of the predict fails to validate Marshalling of item/return values \u00b6 It is possible to specify a pydantic.BaseModel subtype as a type argument for Model classes. This will actually change the structure of the data that is fed into to the _predict_[one|multiple] method. That is, even though the predict is called with dictionnaries, the _predict_[one|multiple] method is guaranteed to be called with an instance of the defined pydantic structure for item, and similarly for the output. For example: class ItemModel ( pydantic . BaseModel ): x : int class ReturnModel ( pydantic . BaseModel ): x : int class SomeValidatedModel ( Model [ ItemModel , ReturnModel ]): def _predict ( self , item ): # item is guaranteed to be an instance of `ItemModel` even if we feed a dictionary item return { \"x\" : item . x } m = SomeValidatedModel () # although we return a dict from the _predict method, return value # is turned into a `ReturnModel` instance. y : ReturnModel = m ({ \"x\" : 1 }) # which also works with lists of items y : List [ ReturnModel ] = m ([{ \"x\" : 1 }, { \"x\" : 2 }]) Batching and vectorization \u00b6 Model can easily deal with a single item or multiple items in the inputs: class Identity ( Model ): def _predict ( self , item ): return item m = Identity () m ({}) # == {} # or m . predict_batch ([{}, { \"hello\" : \"world\" }]) == [{}, { \"hello\" : \"world\" }] It is sometimes interesting to implement batched or vectorized logic, to treat batches of inputs at once. In this case, one can override _predict_batch instead of _predict : class IdentityBatched ( Model ): def _predict ( self , item ): return item m_batched = IdentityBatched () Requesting model predictions will lead to the exact same result: m_batched ({}) # == {} m_batched . predict_batch ([{}, { \"hello\" : \"world\" }]) == [{}, { \"hello\" : \"world\" }] Batch size \u00b6 When _predict_muliple is overridden, the Model will call it with lists of items. The length of the list is fixed and controled by the batch_size parameter either at call time: m_batched . predict_batch ([{}, { \"hello\" : \"world\" }], batch_size = 2 ) or set in the model configuration class IdentityBatched ( Model ): CONFIGURATIONS = { \"some_model\" : { \"model_settings\" : { \"batch_size\" : 32 } } } def _predict ( self , item ): return item Asset class \u00b6 It is sometimes useful for a given asset in memory to serve many different Model objects. It is possibly by using the model_dependencies to point to a parent Model that is the only one to load the asset via _load . In this case, we may not want the parent asset-bearing Model object to implement predict at all. This is what an modelkit.core.model.Asset is. Note In fact, it is defined the other way around: Model s are Asset s with a predict function, and thus Model inherits from Asset . Note There are two ways to use a data asset in a Model : either load it directly via its configuration and the _load , or package it in an Asset and use the deserialized object via model dependencies. Asset file override for debugging \u00b6 There are two ways to override the asset file for a model: You can force to use a local or a remote (e.g. on object store) file for a model by setting an environment variable: modelkit_{}_FILE\".format(model_asset.upper()) . It is also possible to set this programmatically: service = ModelLibrary ( required_models = { \"my_favorite_model\" :{ \"asset_path\" : \"/path/to/asset\" } } ) DistantHTTPModel \u00b6 Sometimes models will simply need to call another microservice, in this case DistantHTTPModel are the way to go. They are instantiated with a POST endpoint URL. class SomeDistantHTTPModel ( DistantHTTPModel ): CONFIGURATIONS = { \"some_model\" : { \"model_settings\" : { \"endpoint\" : \"http://127.0.0.1:8000/api/path/endpoint\" , \"async_mode\" : False } } } When predict[_async] is called, an asynchronous request is made to the http://127.0.0.1:8000/api/path/endpoint with the complete input item serialized in the body and the response of the server is returned. This model supports asynchronous and synchronous requests to the distance model (using either requests or aiohttp ). If the model setting async_mode is unset, each predict call will determine whether it is called in an asychronous environment and choose the best option. If it is set, then all calls will use the same strategy. In addition, it is possible to set this behavior at the level of the ModelLibrary by either setting the async_mode setting in the LibrarySettings or by setting the environment variable modelkit_ASYNC_MODE .","title":"Developing models"},{"location":"library/developing_modelkit.html#models-and-assets","text":"A modelkit.Model is a Python class implementing methods to deserialize assets (stored in the object store), and use them to make prediction. Some quick Model facts: Models do not have to have an asset (text cleaning models) Models do not have to have a prediction method (they are then called Asset ) Models can depend on other models and share objects in memory (in particular, they can share an Asset ) Models can implement batched/vectorized logic Models can implement asynchronous logic and be called either way Models can implement the logic to fit themselves and generate the asset for prediction","title":"Models and assets"},{"location":"library/developing_modelkit.html#the-model-class","text":"modelkit models are subclasses of the modelkit.core.model.Model class. The prediction logic is implemented in an asynchronous _predict method that takes a single argument item . This represents a single item, which is usually a json serializable dict (with maybe numpy arrays). In fact, Models implement _predict or _predict_batch (both async) methods, and Model appropriately chooses between them and batches. The asset loading logic is implemented in a _load method that is run after the Model is instantiated, and can load the asset specified in the Model 's configuration. For more on this see Lazy Mode.","title":"The Model class"},{"location":"library/developing_modelkit.html#the-simplest-model-class","text":"This simple model class will always return \"something\" : from modelkit.core.model import Model class SimpleModel ( Model ): def _predict ( self , item ) -> str : return \"something\" It can be loaded to make \"predictions\" as so: m = SimpleModel () m ({}) # returns \"something\"","title":"The simplest Model class"},{"location":"library/developing_modelkit.html#model-configurations","text":"As models become more complicated they are attached to different assets or other models. We will need to instanciate them through the ModelLibrary object which will take care of all this for us. To do so, we have to configure our model: give it a name and dependencies. Models are made available to clients using modelkit by specifying them using the CONFIGURATIONS class attribute: class SimpleModel ( Model ): CONFIGURATIONS = { \"simple\" : {} } def _predict ( self , item ): return \"something\" Right now, we have only given it a name \"simple\" which makes the model available to other models via the ModelLibrary . The rest of the configuration is empty but we will add to it at the next section. Assuming that SimpleModel is defined in my_module.my_models , it is now accessible via: from modelkit.core import ModelLibrary import my_module.my_models p = ModelLibrary ( models = my_module . my_models ) m = p . get ( \"simple\" ) See Organization for more information on how to organize your models.","title":"Model configurations"},{"location":"library/developing_modelkit.html#model-settings","text":"The simplest configuration options are model_settings : from modelkit.core.model import Model class SimpleModel ( Model ): CONFIGURATIONS = { \"simple\" : { \"model_settings\" : { \"value\" : \"something\" }}, \"simple2\" : { \"model_settings\" : { \"value\" : \"something2\" }} } def _predict ( self , item ): return self . model_settings [ \"value\" ] Now, there are two versions of the model available, simple and simple2 : from modelkit.core import ModelLibrary p = ModelLibrary ( models = SimpleModel ) m = p . get ( \"simple\" ) print ( m ({})) m2 = p . get ( \"simple2\" ) print ( m2 ({})) It will print both \"something\" and \"something2\" .","title":"Model settings"},{"location":"library/developing_modelkit.html#model-assets-and-dependencies","text":"The usefulness of modelkit Model s and their configuration is more apparent when they depend on assets or other models.","title":"Model assets and dependencies"},{"location":"library/developing_modelkit.html#model-assets","text":"A model can implement a _load method that loads information from an asset stored locally, or retrieved from an object store at run time. It may contain files, folders, parameters, optimized data structures, or anything really. The model asset is specified in the CONFIGURATIONS with asset=asset_name:version , following storage.AssetsManager conventions (see Assets ). When the _load method is called, the object will have an asset_path attribute that points to the path of the asset locally. This is then used to load the relevant information from the asset file(s).","title":"Model assets"},{"location":"library/developing_modelkit.html#model-with-asset-example","text":"Adding the key to the model's configuration: class ModelWithAsset ( Model ): CONFIGURATIONS = { \"model_with_asset\" : { \"asset\" : \"test/yolo:1\" } } Will cause the ModelLibrary to download gs://bucket/assets/test/1/yolo locally to the assets folder folder (which storage provider and bucket is used depends on your configuration), and set the Model.asset_path attribute accordingly. It is up to the user to define the deserialization logic: class ModelWithAsset ( Model ): def _load ( self ): # For example, here, the asset is a BZ2 compressed JSON file with bz2 . BZ2File ( self . asset_path , \"rb\" ) as f : self . data_structure = pickle . load ( f ) # loads {\"response\": \"YOLO les simpsons\"} def _predict ( self , item : Dict [ str , str ], ** kwargs ) -> float : return self . data_structure [ \"response\" ] # returns \"YOLO les simpsons\" Note Assets retrieval is handled by modelkit , and it is guaranteed that they be present when _load is called, even in lazy mode. However, it is not true when __init__ is called. In general, it is not a good idea to subclass __init__ .","title":"Model with asset example"},{"location":"library/developing_modelkit.html#model-dependencies","text":"In addition, modelkit models are composable . That is, a Model can depend on other Model s, and exploit their attributes and predictions. For example your can set your model's configuration to have access to two other Model objects: class SomeModel ( Model ): CONFIGURATIONS = { \"some_model\" : { \"model_dependencies\" : { \"sentence_piece_cleaner\" , \"sentence_piece_vectorizer\" } } } The ModelLibrary ensures that whenever _load or the _predict_* function are called, these models are loaded and present in the model_dependencies dictionary: def _predict ( self , item ): cleaned = self . models_dependencies [ \"sentence_piece_cleaner\" ]( item [ \"text\" ]) ... In addition, it is possible to rename dependencies on the fly by providing a mapping to model_dependencies : class SomeModel ( Model ): CONFIGURATIONS = { \"some_model\" : { \"model_dependencies\" : { \"cleaner\" : \"sentence_piece_cleaner\" , } }, \"some_model_2\" : { \"model_dependencies\" : { \"cleaner\" : \"sentence_piece_cleaner_2 } } } In this case, the instantiated SomeModel.model_dependencies[\"cleaner\"] will point to sentence_piece_cleaner in one configuration and to sentence_piece_cleaner_2 in the other.","title":"Model dependencies"},{"location":"library/developing_modelkit.html#model-attributes","text":"Model have several attributes set by the ModelLibrary when they are initialized: asset_path the path to the asset set in the Model 's configuration configuration_key the key of the model's configuration model_dependencies a dictionary of Model dependencies model_settings the model_settings as passed at initialization service_settings the settings of the ModelLibrary that created the model And a set of attributes always present: model_classname the name of the Model 's subclass batch_size the batch size for the model: if _predict_batch is implemented it will always get batches of this size (defaults to 64)","title":"Model attributes"},{"location":"library/developing_modelkit.html#model-typing","text":"It is also possible to provide types for a Model subclass, such that linters and callers know exactly which item type is expected, and what the result of a Model call look like. Types are specified when instantiating the Model class: # This model takes `str` items and always returns `int` values class SomeTypedModel ( Model [ str , int ]): def _predict ( self , item ): return len ( item )","title":"Model typing"},{"location":"library/developing_modelkit.html#static-type-checking","text":"Setting Model types allows static type checkers to fail if the expected return value of calls to predict have the wrong types. Consider the above model: m = SomeTypedModel () x : int = m ( \"ok\" ) y : List [ int ] = m ([ \"ok\" , \"boomer\" ]) z : int = m ( 1 ) # would lead to a typing error with typecheckers (e.g. mypy)","title":"Static type checking"},{"location":"library/developing_modelkit.html#runtime-type-validation","text":"In addition, whenever the model's predict method is called, the type of the item is validated against the provided type and raises an error if the validation fails: modelkit.core.model.ItemValidationException if the item fails to validate modelkit.core.model.ReturnValueValidationException if the return value of the predict fails to validate","title":"Runtime type validation"},{"location":"library/developing_modelkit.html#marshalling-of-itemreturn-values","text":"It is possible to specify a pydantic.BaseModel subtype as a type argument for Model classes. This will actually change the structure of the data that is fed into to the _predict_[one|multiple] method. That is, even though the predict is called with dictionnaries, the _predict_[one|multiple] method is guaranteed to be called with an instance of the defined pydantic structure for item, and similarly for the output. For example: class ItemModel ( pydantic . BaseModel ): x : int class ReturnModel ( pydantic . BaseModel ): x : int class SomeValidatedModel ( Model [ ItemModel , ReturnModel ]): def _predict ( self , item ): # item is guaranteed to be an instance of `ItemModel` even if we feed a dictionary item return { \"x\" : item . x } m = SomeValidatedModel () # although we return a dict from the _predict method, return value # is turned into a `ReturnModel` instance. y : ReturnModel = m ({ \"x\" : 1 }) # which also works with lists of items y : List [ ReturnModel ] = m ([{ \"x\" : 1 }, { \"x\" : 2 }])","title":"Marshalling of item/return values"},{"location":"library/developing_modelkit.html#batching-and-vectorization","text":"Model can easily deal with a single item or multiple items in the inputs: class Identity ( Model ): def _predict ( self , item ): return item m = Identity () m ({}) # == {} # or m . predict_batch ([{}, { \"hello\" : \"world\" }]) == [{}, { \"hello\" : \"world\" }] It is sometimes interesting to implement batched or vectorized logic, to treat batches of inputs at once. In this case, one can override _predict_batch instead of _predict : class IdentityBatched ( Model ): def _predict ( self , item ): return item m_batched = IdentityBatched () Requesting model predictions will lead to the exact same result: m_batched ({}) # == {} m_batched . predict_batch ([{}, { \"hello\" : \"world\" }]) == [{}, { \"hello\" : \"world\" }]","title":"Batching and vectorization"},{"location":"library/developing_modelkit.html#batch-size","text":"When _predict_muliple is overridden, the Model will call it with lists of items. The length of the list is fixed and controled by the batch_size parameter either at call time: m_batched . predict_batch ([{}, { \"hello\" : \"world\" }], batch_size = 2 ) or set in the model configuration class IdentityBatched ( Model ): CONFIGURATIONS = { \"some_model\" : { \"model_settings\" : { \"batch_size\" : 32 } } } def _predict ( self , item ): return item","title":"Batch size"},{"location":"library/developing_modelkit.html#asset-class","text":"It is sometimes useful for a given asset in memory to serve many different Model objects. It is possibly by using the model_dependencies to point to a parent Model that is the only one to load the asset via _load . In this case, we may not want the parent asset-bearing Model object to implement predict at all. This is what an modelkit.core.model.Asset is. Note In fact, it is defined the other way around: Model s are Asset s with a predict function, and thus Model inherits from Asset . Note There are two ways to use a data asset in a Model : either load it directly via its configuration and the _load , or package it in an Asset and use the deserialized object via model dependencies.","title":"Asset class"},{"location":"library/developing_modelkit.html#asset-file-override-for-debugging","text":"There are two ways to override the asset file for a model: You can force to use a local or a remote (e.g. on object store) file for a model by setting an environment variable: modelkit_{}_FILE\".format(model_asset.upper()) . It is also possible to set this programmatically: service = ModelLibrary ( required_models = { \"my_favorite_model\" :{ \"asset_path\" : \"/path/to/asset\" } } )","title":"Asset file override for debugging"},{"location":"library/developing_modelkit.html#distanthttpmodel","text":"Sometimes models will simply need to call another microservice, in this case DistantHTTPModel are the way to go. They are instantiated with a POST endpoint URL. class SomeDistantHTTPModel ( DistantHTTPModel ): CONFIGURATIONS = { \"some_model\" : { \"model_settings\" : { \"endpoint\" : \"http://127.0.0.1:8000/api/path/endpoint\" , \"async_mode\" : False } } } When predict[_async] is called, an asynchronous request is made to the http://127.0.0.1:8000/api/path/endpoint with the complete input item serialized in the body and the response of the server is returned. This model supports asynchronous and synchronous requests to the distance model (using either requests or aiohttp ). If the model setting async_mode is unset, each predict call will determine whether it is called in an asychronous environment and choose the best option. If it is set, then all calls will use the same strategy. In addition, it is possible to set this behavior at the level of the ModelLibrary by either setting the async_mode setting in the LibrarySettings or by setting the environment variable modelkit_ASYNC_MODE .","title":"DistantHTTPModel"},{"location":"library/model_library.html","text":"The ModelLibrary is the primary object that provides predictions from models. ModelLibrary \u00b6 ModelLibrary objects can have a number of settings, passed as a dictionary upon initialization ModelLibrary(required_models = ..., settings = ...) . These parameters are exploited by the ModelLibrary directly and set as the service_settings attribute of Model objects. Main arguments: models a module, Model (or list of either) which is used to find configurations of the models to be loaded configuration allows you to provide an explicit configuration to override the ones present in the Model.CONFIGURATIONS attributes. required_models a list of models to load by the prediction service. This allows you to restrict the models that will actually be loaded onto memory. By default all models are loaded ( required_models=None ), pass the empty list to not load any models (or use the lazy mode). Names in this list have to be defined in the configurations of the models passed via models . You can pass a dictionary to override the asset for each model (see here ). Additionally, the ModelLibrary takes a settings keyword argument which allows you to provide advanced settings: Model instance to be created for the required models. This is useful to download the assets for example with TF serving - lazy_loading : when True, this will cause the assets to be loaded lazily. This is useful for pyspark jobs with model object that are not serializable - enable_tf_serving , tf_serving_timeout_s , tf_serving_port , tf_serving_host : Set parameters related to the serving of TF models (see here ). - assetsmanager_settings : Parameters passed to the assets.manager.AssetsManager - override_storage_prefix : Specify a side prefix from which the prediction service will try to download assets before falling back to classic storage_prefix. It is used to test new assets without having to push them in the main assets prefix (do not use in production). Lazy loading \u00b6 Usually, all model assets are loaded as soon as the ModelLibrary is instantiated. Sometimes this is not desirable, notably when using PySpark. Thus, when lazy_loading=True the ModelLibrary tries to delay the loading and deserialization of the assets as much as possible. You can also set this behavior by setting MODELKIT_LAZY_LOADING=True in your environment. Specifically: When the ModelLibrary is instantiated nothing really happens: the Model object is instantiated without deserializing the asset. When ModelLibrary.get is called the first time, the Model 's asset is downloaded (via ModelLibrary._load ) to a local directory and deserialized. It is also possible to explicitly ask the ModelLibrary to load all required_models at once by calling ModelLibrary.preload . Prediction caching \u00b6 It is possible to use a redis caching mechanism to cache all calls to predict for a ModelLibrary using the enable_redis_cache , cache_host , and cache_port settings of the LibrarySettings . This has to be enabled for each model by setting the cache_predictions model setting to True . The caching works on individual items, before making a prediction with the methods in the Model class, it will attempt to see if an available prediction is already available in the cache. Predictions in the cache are keyed by a hash of the passed item alongside the key of the model (the key used in the configuration of the model). When a prediction on a batch of items is requested, the Model will sieve through each item and attempt to find cached predictions for each. It will therefore only recompute predictions for the select items that do not appear in the cache.","title":"Prediction service"},{"location":"library/model_library.html#modellibrary","text":"ModelLibrary objects can have a number of settings, passed as a dictionary upon initialization ModelLibrary(required_models = ..., settings = ...) . These parameters are exploited by the ModelLibrary directly and set as the service_settings attribute of Model objects. Main arguments: models a module, Model (or list of either) which is used to find configurations of the models to be loaded configuration allows you to provide an explicit configuration to override the ones present in the Model.CONFIGURATIONS attributes. required_models a list of models to load by the prediction service. This allows you to restrict the models that will actually be loaded onto memory. By default all models are loaded ( required_models=None ), pass the empty list to not load any models (or use the lazy mode). Names in this list have to be defined in the configurations of the models passed via models . You can pass a dictionary to override the asset for each model (see here ). Additionally, the ModelLibrary takes a settings keyword argument which allows you to provide advanced settings: Model instance to be created for the required models. This is useful to download the assets for example with TF serving - lazy_loading : when True, this will cause the assets to be loaded lazily. This is useful for pyspark jobs with model object that are not serializable - enable_tf_serving , tf_serving_timeout_s , tf_serving_port , tf_serving_host : Set parameters related to the serving of TF models (see here ). - assetsmanager_settings : Parameters passed to the assets.manager.AssetsManager - override_storage_prefix : Specify a side prefix from which the prediction service will try to download assets before falling back to classic storage_prefix. It is used to test new assets without having to push them in the main assets prefix (do not use in production).","title":"ModelLibrary"},{"location":"library/model_library.html#lazy-loading","text":"Usually, all model assets are loaded as soon as the ModelLibrary is instantiated. Sometimes this is not desirable, notably when using PySpark. Thus, when lazy_loading=True the ModelLibrary tries to delay the loading and deserialization of the assets as much as possible. You can also set this behavior by setting MODELKIT_LAZY_LOADING=True in your environment. Specifically: When the ModelLibrary is instantiated nothing really happens: the Model object is instantiated without deserializing the asset. When ModelLibrary.get is called the first time, the Model 's asset is downloaded (via ModelLibrary._load ) to a local directory and deserialized. It is also possible to explicitly ask the ModelLibrary to load all required_models at once by calling ModelLibrary.preload .","title":"Lazy loading"},{"location":"library/model_library.html#prediction-caching","text":"It is possible to use a redis caching mechanism to cache all calls to predict for a ModelLibrary using the enable_redis_cache , cache_host , and cache_port settings of the LibrarySettings . This has to be enabled for each model by setting the cache_predictions model setting to True . The caching works on individual items, before making a prediction with the methods in the Model class, it will attempt to see if an available prediction is already available in the cache. Predictions in the cache are keyed by a hash of the passed item alongside the key of the model (the key used in the configuration of the model). When a prediction on a batch of items is requested, the Model will sieve through each item and attempt to find cached predictions for each. It will therefore only recompute predictions for the select items that do not appear in the cache.","title":"Prediction caching"},{"location":"library/organizing.html","text":"Model organization \u00b6 modelkit encourages you to organise models in python packages which can be tested and shared between members of the same team. A typical modelkit model repository follows the same organisation as any other Python package. PYTHONPATH \u2514\u2500\u2500 my_ml_package | \u251c\u2500\u2500 __init__.py | \u251c\u2500\u2500 module.py | \u251c\u2500\u2500 module_2.py # defines sub_model | \u251c\u2500\u2500 subpackage | \u2502 \u251c\u2500\u2500 __init__.py | \u2502 \u251c\u2500\u2500 sub_module.py | \u2502 | ... | \u2502 ... modelkit can make all packages available in a single ModelLibrary as so: from modelkit import ModelLibrary import my_ml_package service = ModelLibrary ( models = package ) Note It is also possible to refer to a sub module ModelLibrary(models=package.subpackage) the Model classes themselves ModelLibrary(models=SomeModelClass) , string package names ModelLibrary(models=\"package.module_2\") or any combination of the above ModelLibrary(models=[package.subpackage, SomeModelClass]) In order to restrict the models that are actually being loaded, pass a list of required_models keys to the ModelLibrary instantiation: service = ModelLibrary ( models = [ package . module_2 , package . subpackage ], required_models = [ \"some_model\" ] )","title":"Organizing models"},{"location":"library/organizing.html#model-organization","text":"modelkit encourages you to organise models in python packages which can be tested and shared between members of the same team. A typical modelkit model repository follows the same organisation as any other Python package. PYTHONPATH \u2514\u2500\u2500 my_ml_package | \u251c\u2500\u2500 __init__.py | \u251c\u2500\u2500 module.py | \u251c\u2500\u2500 module_2.py # defines sub_model | \u251c\u2500\u2500 subpackage | \u2502 \u251c\u2500\u2500 __init__.py | \u2502 \u251c\u2500\u2500 sub_module.py | \u2502 | ... | \u2502 ... modelkit can make all packages available in a single ModelLibrary as so: from modelkit import ModelLibrary import my_ml_package service = ModelLibrary ( models = package ) Note It is also possible to refer to a sub module ModelLibrary(models=package.subpackage) the Model classes themselves ModelLibrary(models=SomeModelClass) , string package names ModelLibrary(models=\"package.module_2\") or any combination of the above ModelLibrary(models=[package.subpackage, SomeModelClass]) In order to restrict the models that are actually being loaded, pass a list of required_models keys to the ModelLibrary instantiation: service = ModelLibrary ( models = [ package . module_2 , package . subpackage ], required_models = [ \"some_model\" ] )","title":"Model organization"},{"location":"library/overview.html","text":"Overview \u00b6 The main concepts in modelkit are Model and the ModelLibrary . The ModelLibrary instantiates and configures Model objects and keep track of them during execution. Model objects can then be requested via ModelLibrary.get , and used to make predictions via Model . The ML logic is written in each Model 's predict functions, typically inside a module. Quickstart \u00b6 The normal way to use modelkit models is by instantiating a ModelLibrary with a set of models. from modelkit import ModelLibrary , Model class MyModel ( Model ): CONFIGURATIONS = { \"my_favorite_model\" : {}} def _predict ( self , item ): return item # Create the model library # This downloads all the assets and instantiates all the `Model` # objects that were specified library = ModelLibrary ( models = MyModel ) # This is only a dictionary lookup model = library . get ( \"my_favorite_model\" ) model ( \"hello world\" ) # returns hello world Model are much more powerful than this, in the tutorial you will learn that: Models can have an asset linked to them, to store parameters, weights, or anything. This asset is loaded and deserialized when the ModelLibrary isntantiates the object. Models can depend on other models and share objects in memory (in particular, they can share assets). Only the minimal subset of models is loaded when a given model is required. Model inputs and outputs can be systematically validated using pydantic Models can implement vectorized logic to make faster predictions. Models can implement asynchronous logic and be called either way. Models can serve Tensorflow models conveniently","title":"Models"},{"location":"library/overview.html#overview","text":"The main concepts in modelkit are Model and the ModelLibrary . The ModelLibrary instantiates and configures Model objects and keep track of them during execution. Model objects can then be requested via ModelLibrary.get , and used to make predictions via Model . The ML logic is written in each Model 's predict functions, typically inside a module.","title":"Overview"},{"location":"library/overview.html#quickstart","text":"The normal way to use modelkit models is by instantiating a ModelLibrary with a set of models. from modelkit import ModelLibrary , Model class MyModel ( Model ): CONFIGURATIONS = { \"my_favorite_model\" : {}} def _predict ( self , item ): return item # Create the model library # This downloads all the assets and instantiates all the `Model` # objects that were specified library = ModelLibrary ( models = MyModel ) # This is only a dictionary lookup model = library . get ( \"my_favorite_model\" ) model ( \"hello world\" ) # returns hello world Model are much more powerful than this, in the tutorial you will learn that: Models can have an asset linked to them, to store parameters, weights, or anything. This asset is loaded and deserialized when the ModelLibrary isntantiates the object. Models can depend on other models and share objects in memory (in particular, they can share assets). Only the minimal subset of models is loaded when a given model is required. Model inputs and outputs can be systematically validated using pydantic Models can implement vectorized logic to make faster predictions. Models can implement asynchronous logic and be called either way. Models can serve Tensorflow models conveniently","title":"Quickstart"},{"location":"library/testing.html","text":"Testing modelkit models \u00b6 modelkit provides helper functions to test modelkit.Model instances with pytest . The belief is that test cases for models constitute essential documentation for developers and as a result should appear close to the code of the model itself, much like item/result typing. ModelLibrary fixture and autotesting \u00b6 modelkit.core.fixtures.make_modellibrary_test creates a ModelLibrary fixture and a test that can be used in your pytest testing suite. Call the following in a test file discoverable by pytest: from modelkit.core.fixtures import make_modellibrary_test make_modellibrary_test ( ** model_library_arguments , # insert any arguments to ModelLibrary here fixture_name = \"testing_model_library\" , test_name = \"test_auto_model_library\" , ) This will create a pytest fixture called testing_model_library that returns ModelLibrary(**model_library_arguments) which you can freely reuse. In addition, it creates a test called test_auto_model_library that iterates through the tests defined as part of Model classes. Defining test cases \u00b6 Any modelkit.core.Model can define its own test cases which are discoverable by the test created by make_modellibrary_test : class TestableModel ( Model [ ModelItemType , ModelItemType ]): CONFIGURATIONS : Dict [ str , Dict ] = { \"some_model\" : {}} TEST_CASES = { \"cases\" : [ { \"item\" : { \"x\" : 1 }, \"result\" : { \"x\" : 1 }}, { \"item\" : { \"x\" : 2 }, \"result\" : { \"x\" : 2 }}, ] } def _predict ( self , item ): return item Each test is instantiated with an item value and a result value, the automatic test will iterate through them and run the equivalent of: @pytest . mark . parametrize ( \"model_key, item, result\" , [ case for case in Model . TEST_CASES ]) def test_function ( model_key , item , result , testing_model_library ): lib = testing_model_library . getfixturevalue ( fixture_name ) assert lib . get ( model_key )( item ) == result The easiest way to carry out test cases in interactive programming (ipython, jupyter notebook etc.) is to use the .test() method inherited from BaseModel. This way, one could easily test its brand new model: # Define your brand new model from modelkit import Model class NOTModel ( Model ): CONFIGURATIONS = { \"not_model\" : {}} TEST_CASES = { \"cases\" : [ { \"item\" : True , \"result\" : False }, { \"item\" : False , \"result\" : False } # this should raise an error ] } def predict ( self , item : bool , ** _ ) -> bool : return not item # Execute tests NOTModel () . test () TEST 1 : SUCCESS TEST 2 : FAILED test failed on item item = False expected = False result = True","title":"Testing"},{"location":"library/testing.html#testing-modelkit-models","text":"modelkit provides helper functions to test modelkit.Model instances with pytest . The belief is that test cases for models constitute essential documentation for developers and as a result should appear close to the code of the model itself, much like item/result typing.","title":"Testing modelkit models"},{"location":"library/testing.html#modellibrary-fixture-and-autotesting","text":"modelkit.core.fixtures.make_modellibrary_test creates a ModelLibrary fixture and a test that can be used in your pytest testing suite. Call the following in a test file discoverable by pytest: from modelkit.core.fixtures import make_modellibrary_test make_modellibrary_test ( ** model_library_arguments , # insert any arguments to ModelLibrary here fixture_name = \"testing_model_library\" , test_name = \"test_auto_model_library\" , ) This will create a pytest fixture called testing_model_library that returns ModelLibrary(**model_library_arguments) which you can freely reuse. In addition, it creates a test called test_auto_model_library that iterates through the tests defined as part of Model classes.","title":"ModelLibrary fixture and autotesting"},{"location":"library/testing.html#defining-test-cases","text":"Any modelkit.core.Model can define its own test cases which are discoverable by the test created by make_modellibrary_test : class TestableModel ( Model [ ModelItemType , ModelItemType ]): CONFIGURATIONS : Dict [ str , Dict ] = { \"some_model\" : {}} TEST_CASES = { \"cases\" : [ { \"item\" : { \"x\" : 1 }, \"result\" : { \"x\" : 1 }}, { \"item\" : { \"x\" : 2 }, \"result\" : { \"x\" : 2 }}, ] } def _predict ( self , item ): return item Each test is instantiated with an item value and a result value, the automatic test will iterate through them and run the equivalent of: @pytest . mark . parametrize ( \"model_key, item, result\" , [ case for case in Model . TEST_CASES ]) def test_function ( model_key , item , result , testing_model_library ): lib = testing_model_library . getfixturevalue ( fixture_name ) assert lib . get ( model_key )( item ) == result The easiest way to carry out test cases in interactive programming (ipython, jupyter notebook etc.) is to use the .test() method inherited from BaseModel. This way, one could easily test its brand new model: # Define your brand new model from modelkit import Model class NOTModel ( Model ): CONFIGURATIONS = { \"not_model\" : {}} TEST_CASES = { \"cases\" : [ { \"item\" : True , \"result\" : False }, { \"item\" : False , \"result\" : False } # this should raise an error ] } def predict ( self , item : bool , ** _ ) -> bool : return not item # Execute tests NOTModel () . test () TEST 1 : SUCCESS TEST 2 : FAILED test failed on item item = False expected = False result = True","title":"Defining test cases"},{"location":"library/using_models.html","text":"Consuming modelkit models \u00b6 Loading models \u00b6 Simple case \u00b6 Simple Model objects can be created and instantiated straight away from modelkit import Model class MyModel ( Model ): def _predict ( self , item ): return item m = MyModel () In this case, however, the Model does not have any model_dependencies or asset . General case \u00b6 In general, to resolve assets and dependencies, modelkit models need to be instantiated using ModelLibrary with a set of models. Models are then accessed via ModelLibrary.get(\"some name\") . For example, to load a model that has one dependency: from modelkit import ModelLibrary , Model class MyModel ( Model ): CONFIGURATIONS = { \"a_model\" : {} } def _predict ( self , item ): return item class MyComposedModel ( Model ): CONFIGURATIONS = { \"my_favorite_model\" : { \"model_dependencies\" : { \"a_model\" } } } def _predict ( self , item ): return item # Create the model library # This downloads all the assets and instantiates all the `Model` # objects that were specified library = ModelLibrary ( models = [ MyModel , MyComposedModel ]) # This is only a dictionary lookup model = library . get ( \"my_favorite_model\" ) From a package \u00b6 modelkit encourages you to store your models in a Python package (see Organization ). For example, assuming we have a modelkit model configured as my_favorite_model somewhere under the my_models module. from modelkit import ModelLibrary import my_models # contains subclasses of `Model` # Create the library # This downloads assets and instantiates model_dependencies library = ModelLibrary ( models = my_models ) model = library . get ( \"my_favorite_model\" ) Shortcuts For development, it is also possible to load a single model without a ModelLibrary : from modelkit import load_model model = load_model ( \"my_favorite_model\" , models = \"my_models\" ) If you have set the MODELKIT_DEFAULT_PACKAGE environment variable, you can also skip the models=... part. Getting model predictions \u00b6 Predictions for single items \u00b6 Predictions can be obtained by calling the object: prediction = model ( item ) # or model.predict(item) This will call whichever one of _predict or _predict_batch was implemented in the Model . Predictions for lists of items \u00b6 Predictions for list of items can be obtained by using predict_batch : predictions = model . predict_batch ( items ) This will call whichever one of _predict or _predict_batch was implemented in the Model . But in the case in which _predict_batch is implemented, you may see speed ups due to vectorization. Example In this example, we implement a dummy Model that computes the position of the min in a list using np.argmin . In one version the code is not vectorized (it operates on a single item) and in the other one it is (a whole batched is processed at once). The vectorized version is ~50% faster import random import timeit from modelkit.core.model import Model import numpy as np # Create some data data = [] base_item = list ( range ( 100 )) for _ in range ( 128 ): random . shuffle ( base_item ) data . append ( list ( base_item )) # This model is not vectorized, `np.argmin` # will be called individually for each batch class MinModel ( Model ): def _predict ( self , item ): return np . argmin ( item ) m = MinModel () # This model is vectorized, `np.argmin` # is called over a whole batch class MinBatchedModel ( Model ): def _predict_batch ( self , items ): return np . argmin ( items , axis = 1 ) m_batched = MinBatchedModel () # They do return the same results assert m . predict_batch ( data ) == m_batched . predict_batch ( data ) # The batched model is ~50% slower timeit . timeit ( lambda : m . predict_batch ( data ), number = 1000 ) # The batched model is ~50% slower timeit . timeit ( lambda : m_batched . predict_batch ( data ), number = 1000 ) # Even more so with a larger batch size timeit . timeit ( lambda : m_batched . predict_batch ( data , batch_size = 128 ), number = 1000 ) Predictions from iterators \u00b6 It is also possible to iterate through predictions with an iterator, which is convenient to avoid having to load all items to memory before getting predictions. def generate_items (): ... yield item for prediction in model . predict_gen ( generate_items ()): # use prediction ... A typical use case is to iterate through the lines of a file, perform some processing and write it straight back to another file","title":"Getting predictions"},{"location":"library/using_models.html#consuming-modelkit-models","text":"","title":"Consuming modelkit models"},{"location":"library/using_models.html#loading-models","text":"","title":"Loading models"},{"location":"library/using_models.html#simple-case","text":"Simple Model objects can be created and instantiated straight away from modelkit import Model class MyModel ( Model ): def _predict ( self , item ): return item m = MyModel () In this case, however, the Model does not have any model_dependencies or asset .","title":"Simple case"},{"location":"library/using_models.html#general-case","text":"In general, to resolve assets and dependencies, modelkit models need to be instantiated using ModelLibrary with a set of models. Models are then accessed via ModelLibrary.get(\"some name\") . For example, to load a model that has one dependency: from modelkit import ModelLibrary , Model class MyModel ( Model ): CONFIGURATIONS = { \"a_model\" : {} } def _predict ( self , item ): return item class MyComposedModel ( Model ): CONFIGURATIONS = { \"my_favorite_model\" : { \"model_dependencies\" : { \"a_model\" } } } def _predict ( self , item ): return item # Create the model library # This downloads all the assets and instantiates all the `Model` # objects that were specified library = ModelLibrary ( models = [ MyModel , MyComposedModel ]) # This is only a dictionary lookup model = library . get ( \"my_favorite_model\" )","title":"General case"},{"location":"library/using_models.html#from-a-package","text":"modelkit encourages you to store your models in a Python package (see Organization ). For example, assuming we have a modelkit model configured as my_favorite_model somewhere under the my_models module. from modelkit import ModelLibrary import my_models # contains subclasses of `Model` # Create the library # This downloads assets and instantiates model_dependencies library = ModelLibrary ( models = my_models ) model = library . get ( \"my_favorite_model\" ) Shortcuts For development, it is also possible to load a single model without a ModelLibrary : from modelkit import load_model model = load_model ( \"my_favorite_model\" , models = \"my_models\" ) If you have set the MODELKIT_DEFAULT_PACKAGE environment variable, you can also skip the models=... part.","title":"From a package"},{"location":"library/using_models.html#getting-model-predictions","text":"","title":"Getting model predictions"},{"location":"library/using_models.html#predictions-for-single-items","text":"Predictions can be obtained by calling the object: prediction = model ( item ) # or model.predict(item) This will call whichever one of _predict or _predict_batch was implemented in the Model .","title":"Predictions for single items"},{"location":"library/using_models.html#predictions-for-lists-of-items","text":"Predictions for list of items can be obtained by using predict_batch : predictions = model . predict_batch ( items ) This will call whichever one of _predict or _predict_batch was implemented in the Model . But in the case in which _predict_batch is implemented, you may see speed ups due to vectorization. Example In this example, we implement a dummy Model that computes the position of the min in a list using np.argmin . In one version the code is not vectorized (it operates on a single item) and in the other one it is (a whole batched is processed at once). The vectorized version is ~50% faster import random import timeit from modelkit.core.model import Model import numpy as np # Create some data data = [] base_item = list ( range ( 100 )) for _ in range ( 128 ): random . shuffle ( base_item ) data . append ( list ( base_item )) # This model is not vectorized, `np.argmin` # will be called individually for each batch class MinModel ( Model ): def _predict ( self , item ): return np . argmin ( item ) m = MinModel () # This model is vectorized, `np.argmin` # is called over a whole batch class MinBatchedModel ( Model ): def _predict_batch ( self , items ): return np . argmin ( items , axis = 1 ) m_batched = MinBatchedModel () # They do return the same results assert m . predict_batch ( data ) == m_batched . predict_batch ( data ) # The batched model is ~50% slower timeit . timeit ( lambda : m . predict_batch ( data ), number = 1000 ) # The batched model is ~50% slower timeit . timeit ( lambda : m_batched . predict_batch ( data ), number = 1000 ) # Even more so with a larger batch size timeit . timeit ( lambda : m_batched . predict_batch ( data , batch_size = 128 ), number = 1000 )","title":"Predictions for lists of items"},{"location":"library/using_models.html#predictions-from-iterators","text":"It is also possible to iterate through predictions with an iterator, which is convenient to avoid having to load all items to memory before getting predictions. def generate_items (): ... yield item for prediction in model . predict_gen ( generate_items ()): # use prediction ... A typical use case is to iterate through the lines of a file, perform some processing and write it straight back to another file","title":"Predictions from iterators"},{"location":"library/special/distant.html","text":"Tensorflow models \u00b6 modelkit provides different modes to use TF models, and makes it relatively easy to switch between them. calling the TF model using the tensorflow module requesting predictions from TensorFlow Serving synchronously via a REST API requesting predictions from TensorFlow Serving asynchronously via a REST API requesting predictions from TensorFlow Serving synchronously via gRPC TF Serving \u00b6 Running a TF serving container locally \u00b6 In order to run a TF Serving docker locally, one first needs to download the models and write a configuration file. This can be achieved by bin/tf_serving.py configure local-docker [SERVICE] , which will write the configuration file under the local MODELKIT_ASSETS_DIR . SERVICE refers to a key of the modelkit.models.config.MODELKIT_TF_SERVING_MODELS , so essentially describes different subsets of the models that use TF serving. Possible keys are modelkit or dataplayground . The bash script bin/run_tf_serving.sh then runs the docker container exposing both the REST and gRPC endpoints. Also see the CLI documentation . Details \u00b6 The CLI bin/tf_serving.py configure local-docker creates a configuration file for tensorflow serving, with the model locations refered to relative to the container file system . As a result, the TF serving container will expect that the MODELKIT_ASSETS_DIR/modelkit-assets is bound to the /config directory inside the container. The container can then be started by pointing TF serving to the generated configuration file --model_config_file=/config/modelkit.config . This is already achieved by the bin/run_tfserving.sh CLI. See also: the Tensorflow serving documentation the Tensorflow serving github Internal TF serving settings \u00b6 Several parameters control how modelkit requests predictions from TF serving. enable_tf_serving : Controls wether to use TF serving or use TF locally as a lib tf_serving_host : Host to connect to to request TF predictions tf_serving_port : Port to connect to to request TF predictions tf_serving_mode : Can be grpc (with grpc ) or rest (with requests for TensorflowModel , or with aiohttp for AsyncTensorflowModel ) tf_serving_timeout_s : timeout to wait for the first TF serving response All of these parameters can be set programmatically (and passed to the ModelLibrary 's settings), or they are fetched from environment variables (which take precedence), in which case they should be uppercased. TensorflowModel class \u00b6 All tensorflow based models should derive from the TensorflowModel class. This class provides a number of functions that help with loading/serving TF models. At initialization time, a TensorflowModel has to be provided with definitions of the tensors predicted by the TF model: output_tensor_mapping a dict of arbitrary key s to tensor names describing the outputs. output_tensor_shapes and output_tensor_dtypes a dict of shapes and dtypes of these tensors. Typically, inside a _predict_batch , one would do something like: def _predict_batch ( self , items ): data = await self . _tensorflow_predict ( { key : np . stack ([ item [ key ] for item in items ], axis = 0 ) for key in items [ 0 ] } ) # ... return [ { key : data [ key ][ k ] for key in self . output_tensor_mapping } for k in range ( len ( items )) ] Important Be careful that _tensorflow_predict returns a dict of np.ndarray of shape (len(items),?) when _predict_batch expects a list of len(items) dicts of np.ndarray . Other methods to re-implement \u00b6 Post processing \u00b6 After the TF call, _tensorflow_predict_* returns a dict of np.ndarray of shape (len(items),?) . These can be further manipulated by reimplementing the TensorflowModel._post_processing function, e.g. to reshape, change type, select a subset of features. Empty predictions \u00b6 Oftentimes we manipulate the item before feeding it to TF, e.g. doing text cleaning or vectorization. This sometimes results in making the prediction trivial. In this case we use the following pattern, wherein we leverage the method TensorflowModel._rebuild_predictions_with_mask : def _predict_batch ( self , items : List [ Dict [ str , str ]], ** kwargs ) -> List [ Tuple [ np . ndarray , List [ str ]]]: treated_items = self . treat ( items ) # Some of them can be None / empty, let's filter them out before prediction mask = [ x is not None and np . count_nonzero ( x ) > 0 for x in treated_items ] results = [] if any ( mask ): items_to_predict = np . concatenate ( list ( compress ( treated_items , mask ))) results = await self . _tensorflow_predict ( { \"input_tensor\" : items_to_predict } ) # Merge the just-computed predictions with empty vectors for empty items # Making sure everything is well-aligned return self . _rebuild_predictions_with_mask ( mask , results ) Notably, TensorflowModel._rebuild_predictions_with_mask uses TensorflowModel._generate_empty_prediction which returns the prediction expected for empty items TF Serving during tests \u00b6 During tests locally on an OSX machine, TF serving is run locally using the TF serving docker and the docker python library (see the tf_serving fixture in tests/conftest.py ). On the CI/CD pipelines, docker is not available, so we run tensorflow-model-server directly after installing it via apt .","title":"Distant Models"},{"location":"library/special/distant.html#tensorflow-models","text":"modelkit provides different modes to use TF models, and makes it relatively easy to switch between them. calling the TF model using the tensorflow module requesting predictions from TensorFlow Serving synchronously via a REST API requesting predictions from TensorFlow Serving asynchronously via a REST API requesting predictions from TensorFlow Serving synchronously via gRPC","title":"Tensorflow models"},{"location":"library/special/distant.html#tf-serving","text":"","title":"TF Serving"},{"location":"library/special/distant.html#running-a-tf-serving-container-locally","text":"In order to run a TF Serving docker locally, one first needs to download the models and write a configuration file. This can be achieved by bin/tf_serving.py configure local-docker [SERVICE] , which will write the configuration file under the local MODELKIT_ASSETS_DIR . SERVICE refers to a key of the modelkit.models.config.MODELKIT_TF_SERVING_MODELS , so essentially describes different subsets of the models that use TF serving. Possible keys are modelkit or dataplayground . The bash script bin/run_tf_serving.sh then runs the docker container exposing both the REST and gRPC endpoints. Also see the CLI documentation .","title":"Running a TF serving container locally"},{"location":"library/special/distant.html#details","text":"The CLI bin/tf_serving.py configure local-docker creates a configuration file for tensorflow serving, with the model locations refered to relative to the container file system . As a result, the TF serving container will expect that the MODELKIT_ASSETS_DIR/modelkit-assets is bound to the /config directory inside the container. The container can then be started by pointing TF serving to the generated configuration file --model_config_file=/config/modelkit.config . This is already achieved by the bin/run_tfserving.sh CLI. See also: the Tensorflow serving documentation the Tensorflow serving github","title":"Details"},{"location":"library/special/distant.html#internal-tf-serving-settings","text":"Several parameters control how modelkit requests predictions from TF serving. enable_tf_serving : Controls wether to use TF serving or use TF locally as a lib tf_serving_host : Host to connect to to request TF predictions tf_serving_port : Port to connect to to request TF predictions tf_serving_mode : Can be grpc (with grpc ) or rest (with requests for TensorflowModel , or with aiohttp for AsyncTensorflowModel ) tf_serving_timeout_s : timeout to wait for the first TF serving response All of these parameters can be set programmatically (and passed to the ModelLibrary 's settings), or they are fetched from environment variables (which take precedence), in which case they should be uppercased.","title":"Internal TF serving settings"},{"location":"library/special/distant.html#tensorflowmodel-class","text":"All tensorflow based models should derive from the TensorflowModel class. This class provides a number of functions that help with loading/serving TF models. At initialization time, a TensorflowModel has to be provided with definitions of the tensors predicted by the TF model: output_tensor_mapping a dict of arbitrary key s to tensor names describing the outputs. output_tensor_shapes and output_tensor_dtypes a dict of shapes and dtypes of these tensors. Typically, inside a _predict_batch , one would do something like: def _predict_batch ( self , items ): data = await self . _tensorflow_predict ( { key : np . stack ([ item [ key ] for item in items ], axis = 0 ) for key in items [ 0 ] } ) # ... return [ { key : data [ key ][ k ] for key in self . output_tensor_mapping } for k in range ( len ( items )) ] Important Be careful that _tensorflow_predict returns a dict of np.ndarray of shape (len(items),?) when _predict_batch expects a list of len(items) dicts of np.ndarray .","title":"TensorflowModel class"},{"location":"library/special/distant.html#other-methods-to-re-implement","text":"","title":"Other methods to re-implement"},{"location":"library/special/distant.html#post-processing","text":"After the TF call, _tensorflow_predict_* returns a dict of np.ndarray of shape (len(items),?) . These can be further manipulated by reimplementing the TensorflowModel._post_processing function, e.g. to reshape, change type, select a subset of features.","title":"Post processing"},{"location":"library/special/distant.html#empty-predictions","text":"Oftentimes we manipulate the item before feeding it to TF, e.g. doing text cleaning or vectorization. This sometimes results in making the prediction trivial. In this case we use the following pattern, wherein we leverage the method TensorflowModel._rebuild_predictions_with_mask : def _predict_batch ( self , items : List [ Dict [ str , str ]], ** kwargs ) -> List [ Tuple [ np . ndarray , List [ str ]]]: treated_items = self . treat ( items ) # Some of them can be None / empty, let's filter them out before prediction mask = [ x is not None and np . count_nonzero ( x ) > 0 for x in treated_items ] results = [] if any ( mask ): items_to_predict = np . concatenate ( list ( compress ( treated_items , mask ))) results = await self . _tensorflow_predict ( { \"input_tensor\" : items_to_predict } ) # Merge the just-computed predictions with empty vectors for empty items # Making sure everything is well-aligned return self . _rebuild_predictions_with_mask ( mask , results ) Notably, TensorflowModel._rebuild_predictions_with_mask uses TensorflowModel._generate_empty_prediction which returns the prediction expected for empty items","title":"Empty predictions"},{"location":"library/special/distant.html#tf-serving-during-tests","text":"During tests locally on an OSX machine, TF serving is run locally using the TF serving docker and the docker python library (see the tf_serving fixture in tests/conftest.py ). On the CI/CD pipelines, docker is not available, so we run tensorflow-model-server directly after installing it via apt .","title":"TF Serving during tests"},{"location":"library/special/tensorflow.html","text":"Tensorflow models \u00b6 modelkit provides different modes to use TF models, and makes it easy to switch between them. calling the TF model using the tensorflow module requesting predictions from TensorFlow Serving synchronously via a REST API requesting predictions from TensorFlow Serving asynchronously via a REST API requesting predictions from TensorFlow Serving synchronously via gRPC TensorflowModel class \u00b6 All tensorflow based models should derive from the TensorflowModel class. This class provides a number of functions that help with loading/serving TF models. At initialization time, a TensorflowModel has to be provided with definitions of the tensors predicted by the TF model: output_tensor_mapping a dict of arbitrary key s to tensor names describing the outputs. output_tensor_shapes and output_tensor_dtypes a dict of shapes and dtypes of these tensors. Typically, inside a _predict_batch , one would do something like: def _predict_batch ( self , items ): data = await self . _tensorflow_predict ( { key : np . stack ([ item [ key ] for item in items ], axis = 0 ) for key in items [ 0 ] } ) # ... return [ { key : data [ key ][ k ] for key in self . output_tensor_mapping } for k in range ( len ( items )) ] Important Be careful that _tensorflow_predict returns a dict of np.ndarray of shape (len(items),?) when _predict_batch expects a list of len(items) dicts of np.ndarray . Other convenience methods \u00b6 Post processing \u00b6 After the TF call, _tensorflow_predict_* returns a dict of np.ndarray of shape (len(items),?) . These can be further manipulated by reimplementing the TensorflowModel._post_processing function, e.g. to reshape, change type, select a subset of features. Empty predictions \u00b6 Oftentimes we manipulate the item before feeding it to TF, e.g. doing text cleaning or vectorization. This sometimes results in making the prediction trivial. In this case we use the following pattern, wherein we leverage the method TensorflowModel._rebuild_predictions_with_mask : def _predict_batch ( self , items : List [ Dict [ str , str ]], ** kwargs ) -> List [ Tuple [ np . ndarray , List [ str ]]]: treated_items = self . treat ( items ) # Some of them can be None / empty, let's filter them out before prediction mask = [ x is not None and np . count_nonzero ( x ) > 0 for x in treated_items ] results = [] if any ( mask ): items_to_predict = np . concatenate ( list ( compress ( treated_items , mask ))) results = await self . _tensorflow_predict ( { \"input_tensor\" : items_to_predict } ) # Merge the just-computed predictions with empty vectors for empty items # Making sure everything is well-aligned return self . _rebuild_predictions_with_mask ( mask , results ) Notably, TensorflowModel._rebuild_predictions_with_mask uses TensorflowModel._generate_empty_prediction which returns the prediction expected for empty items TF Serving \u00b6 modelkit provides an easy way to query Tensorflow models served via TF Serving. When TF serving is configured, the TF models are not run in the main process, but queried. Running TF serving container locally \u00b6 In order to run a TF Serving docker locally, one first needs to download the models and write a configuration file. This can be achieved by modelkit tf-serving local-docker --models [ PACKAGE ] The CLI creates a configuration file for tensorflow serving, with the model locations refered to relative to the container file system . As a result, the TF serving container will expect that the MODELKIT_ASSETS_DIR is bound to the /config directory inside the container. Specifically, the CLI: Instantiates a ModelLibrary with all configured models in PACKAGE Downloads all necessary assets in the MODELKIT_ASSETS_DIR writes a configuration file under the local MODELKIT_ASSETS_DIR with all TF models that are configured The container can then be started by pointing TF serving to the generated configuration file --model_config_file=/config/config.config : docker run \\ --name local-tf-serving \\ -d \\ -p 8500 :8500 -p 8501 :8501 \\ -v ${ MODELKIT_ASSETS_DIR } :/config \\ -t tensorflow/serving \\ --model_config_file = /config/config.config \\ --rest_api_port = 8501 \\ --port = 8500 See also: the CLI documentation . the Tensorflow serving documentation the Tensorflow serving github Internal TF serving settings \u00b6 Several environment variables control how modelkit requests predictions from TF serving. MODELKIT_TF_SERVING_ENABLE : Controls whether to use TF serving or use TF locally as a lib MODELKIT_TF_SERVING_HOST : Host to connect to to request TF predictions MODELKIT_TF_SERVING_PORT : Port to connect to to request TF predictions MODELKIT_TF_SERVING_MODE : Can be grpc (with grpc ) or rest (with requests for TensorflowModel , or with aiohttp for AsyncTensorflowModel ) TF_SERVING_TIMEOUT_S : timeout to wait for the first TF serving response All of these parameters can be set programmatically (and passed to the ModelLibrary 's settings): lib_serving_grpc = ModelLibrary ( required_models =... , settings = LibrarySettings ( tf_serving = { \"enable\" : True , \"port\" : 8500 , \"mode\" : \"grpc\" , \"host\" : \"localhost\" , } ), models =... , ) Using TF Serving during tests \u00b6 modelkit provides a fixture to run TF serving during testing: @pytest . fixture ( scope = \"session\" ) def tf_serving (): lib = ModelLibrary ( models =... , settings = { \"lazy_loading\" : True }) yield tf_serving_fixture ( request , lib ) This will configure and run TF serving during the test session, provided docker is present.","title":"Tensorflow models"},{"location":"library/special/tensorflow.html#tensorflow-models","text":"modelkit provides different modes to use TF models, and makes it easy to switch between them. calling the TF model using the tensorflow module requesting predictions from TensorFlow Serving synchronously via a REST API requesting predictions from TensorFlow Serving asynchronously via a REST API requesting predictions from TensorFlow Serving synchronously via gRPC","title":"Tensorflow models"},{"location":"library/special/tensorflow.html#tensorflowmodel-class","text":"All tensorflow based models should derive from the TensorflowModel class. This class provides a number of functions that help with loading/serving TF models. At initialization time, a TensorflowModel has to be provided with definitions of the tensors predicted by the TF model: output_tensor_mapping a dict of arbitrary key s to tensor names describing the outputs. output_tensor_shapes and output_tensor_dtypes a dict of shapes and dtypes of these tensors. Typically, inside a _predict_batch , one would do something like: def _predict_batch ( self , items ): data = await self . _tensorflow_predict ( { key : np . stack ([ item [ key ] for item in items ], axis = 0 ) for key in items [ 0 ] } ) # ... return [ { key : data [ key ][ k ] for key in self . output_tensor_mapping } for k in range ( len ( items )) ] Important Be careful that _tensorflow_predict returns a dict of np.ndarray of shape (len(items),?) when _predict_batch expects a list of len(items) dicts of np.ndarray .","title":"TensorflowModel class"},{"location":"library/special/tensorflow.html#other-convenience-methods","text":"","title":"Other convenience methods"},{"location":"library/special/tensorflow.html#post-processing","text":"After the TF call, _tensorflow_predict_* returns a dict of np.ndarray of shape (len(items),?) . These can be further manipulated by reimplementing the TensorflowModel._post_processing function, e.g. to reshape, change type, select a subset of features.","title":"Post processing"},{"location":"library/special/tensorflow.html#empty-predictions","text":"Oftentimes we manipulate the item before feeding it to TF, e.g. doing text cleaning or vectorization. This sometimes results in making the prediction trivial. In this case we use the following pattern, wherein we leverage the method TensorflowModel._rebuild_predictions_with_mask : def _predict_batch ( self , items : List [ Dict [ str , str ]], ** kwargs ) -> List [ Tuple [ np . ndarray , List [ str ]]]: treated_items = self . treat ( items ) # Some of them can be None / empty, let's filter them out before prediction mask = [ x is not None and np . count_nonzero ( x ) > 0 for x in treated_items ] results = [] if any ( mask ): items_to_predict = np . concatenate ( list ( compress ( treated_items , mask ))) results = await self . _tensorflow_predict ( { \"input_tensor\" : items_to_predict } ) # Merge the just-computed predictions with empty vectors for empty items # Making sure everything is well-aligned return self . _rebuild_predictions_with_mask ( mask , results ) Notably, TensorflowModel._rebuild_predictions_with_mask uses TensorflowModel._generate_empty_prediction which returns the prediction expected for empty items","title":"Empty predictions"},{"location":"library/special/tensorflow.html#tf-serving","text":"modelkit provides an easy way to query Tensorflow models served via TF Serving. When TF serving is configured, the TF models are not run in the main process, but queried.","title":"TF Serving"},{"location":"library/special/tensorflow.html#running-tf-serving-container-locally","text":"In order to run a TF Serving docker locally, one first needs to download the models and write a configuration file. This can be achieved by modelkit tf-serving local-docker --models [ PACKAGE ] The CLI creates a configuration file for tensorflow serving, with the model locations refered to relative to the container file system . As a result, the TF serving container will expect that the MODELKIT_ASSETS_DIR is bound to the /config directory inside the container. Specifically, the CLI: Instantiates a ModelLibrary with all configured models in PACKAGE Downloads all necessary assets in the MODELKIT_ASSETS_DIR writes a configuration file under the local MODELKIT_ASSETS_DIR with all TF models that are configured The container can then be started by pointing TF serving to the generated configuration file --model_config_file=/config/config.config : docker run \\ --name local-tf-serving \\ -d \\ -p 8500 :8500 -p 8501 :8501 \\ -v ${ MODELKIT_ASSETS_DIR } :/config \\ -t tensorflow/serving \\ --model_config_file = /config/config.config \\ --rest_api_port = 8501 \\ --port = 8500 See also: the CLI documentation . the Tensorflow serving documentation the Tensorflow serving github","title":"Running TF serving container locally"},{"location":"library/special/tensorflow.html#internal-tf-serving-settings","text":"Several environment variables control how modelkit requests predictions from TF serving. MODELKIT_TF_SERVING_ENABLE : Controls whether to use TF serving or use TF locally as a lib MODELKIT_TF_SERVING_HOST : Host to connect to to request TF predictions MODELKIT_TF_SERVING_PORT : Port to connect to to request TF predictions MODELKIT_TF_SERVING_MODE : Can be grpc (with grpc ) or rest (with requests for TensorflowModel , or with aiohttp for AsyncTensorflowModel ) TF_SERVING_TIMEOUT_S : timeout to wait for the first TF serving response All of these parameters can be set programmatically (and passed to the ModelLibrary 's settings): lib_serving_grpc = ModelLibrary ( required_models =... , settings = LibrarySettings ( tf_serving = { \"enable\" : True , \"port\" : 8500 , \"mode\" : \"grpc\" , \"host\" : \"localhost\" , } ), models =... , )","title":"Internal TF serving settings"},{"location":"library/special/tensorflow.html#using-tf-serving-during-tests","text":"modelkit provides a fixture to run TF serving during testing: @pytest . fixture ( scope = \"session\" ) def tf_serving (): lib = ModelLibrary ( models =... , settings = { \"lazy_loading\" : True }) yield tf_serving_fixture ( request , lib ) This will configure and run TF serving during the test session, provided docker is present.","title":"Using TF Serving during tests"}]}